\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\textwidth=7.6in
\textheight=9.9in
\topmargin=-.9in
\headheight=0in
\headsep=.5in
\hoffset=-1.5in
\setlength\parindent{0pt}

\begin{document}

\begin{center}
    \textbf{Theory of Algorithms Class Notes} \\[0.25ex]
    Calvin Walker
\end{center}

\textbf{Lecture 1: Gale-Shapley Algorithm for Stable Matching} \\[1.0ex]
\underline{Stable Matching Problem}: There are two groups of people: $A = \{a_1 \dots a_n\}$ and $B = \{b_1 \dots b_n\}$, and we want to find a stable matching between $A$ and $B$ such that there is no pair of people $a_i, b_j$ who would rather be with each other than their current partners. i.e We want to find a permutation $\pi$ such that there are no $i, j \in [n]$ for whom
\begin{enumerate}
    \item $\pi(i) \neq j$
    \item $a_i$ prefers $b_j$ to $b_{\pi(i)}$
    \item $b_j$ prefers $a_i$ to $b_{\pi^{-1}(j)}$
\end{enumerate}
% \underline{Availability}: We say $b_j$ is available to $a_i$ if one of the following cases holds: $b_j$ is unmatched or partnered with $a_i'$ such that $b_j$ preferes $a_i$ to $a_i'$ \\[0.5ex]
\underline{Gale-Shapley algorithm}: The people in group $A$ make offers in the order of their preference lists. When $b_j$ receives an offer from $a_i$, if $b_j$ is unmatched or preferes $a_i$ to their current partner, they accept and become partners with $a_i$. Otherwise, $b_j$ rejects and $a_i$ moves onto the next person. \\[0.5ex]
\underline{Properties}:
\begin{itemize}
    \item The Gale-Shapley algorithm terminates in $O(n^2)$ steps and results in a stable matching. \\[0.5ex]
    Proof: There are $n^2$ possible offers and each offer is made at most once. \\[0.35ex]
    \textit{Terminates}: Oberve that if some $a_i$ makes an offer to their last choice $b_j$, then all people except $b_j$ must be unavailable for $a_i$ and thus already matched. There are only $n - 1$ other people in A, so $b_j$ must be unmatched, and $b_j$ accepts $a_i$'s offer, and the algorithm terminates. \\[0.55ex] 
    \textit{Stable}: Assume there are $i,j \in [n]$ such that $a_i$ and $b_j$ prefer each other to their current partners. Let $b_j$ be $a_i'$'s current partner and $b_j'$ be matched with $a_i$. Then $b_j$ is available to $a_i$. However, since $a_i$ is matched with $b_j'$, $b_j$ must have been unavailable to $a_i$ at some point, a contradiction.
    \item As Gale-Shapley progresses, (1) people in B only become happier, and (2) if $b_j$ becomes unavailable to $a_i$ they never become available again. \\[0.5ex]
    Proof: (1) $b_j$ only breaks off from its match if it prefers the new $a_i$.
    (2) Assume $b_j$ becomes unavailable to $a_i$ and then becomes available again. Let $a_i'$ be $b_j$'s partner when $b_j$ became unavailable to $a_i$. Let $a_i''$ be $b_j$'s partner when $b_j$ became available to $a_i$ again. Then, $b_j$ prefers $a_i'$ to $a_i$ and $a_i$ to $a_i''$. However, $b_j$ must prefer $a_i''$ to $a_i'$, a contradiction. 
\end{itemize}
% \begin{algorithm}
% \caption{Gale-Shapley}

% \end{algorithm}
\textbf{Lecture 2: Big O Notation} \\[1.0ex]
\underline{Big O Notation}: Given functions $f, g: \mathbb{N} \mapsto \mathbb{R}^+$
\begin{enumerate}
    \item We say that $f(n)$ is $O(g(n))$ if there exist $n_0, C > 0$ such that for all $n \geq n_0$, $f(n) \leq Cg(n)$
    \item We say that $f(n)$ is $\Omega(g(n))$ if there exist $n_0, c > 0$ such that for all $n \geq n_0$, $f(n) \geq c g(n)$
    \item We say that $f(n)$ is $\Theta(g(n))$ if $f(n)$ is $O(g(n))$ and $\Omega(g(n))$ such that there exist $n_0, c, C'$ such that for all $n \geq n_0$, $cg(n) \leq f(n) \leq C'g(n)$
\end{enumerate}
\underline{Examples}: 
\begin{itemize}
    \item If $f(n) \leq 8n\log_2(n) + 20n + 100$ then $f(n)$ is $O(n\log n)$
    \item If $f(n) \geq n^2 - 3n - 2$ then $f(n)$ is $\Omega(n^2)$
    \item If $\frac{1}{2}\log_2 n - 2 \leq f(n) \leq 4 \log_2 n + 1$ then $f(n)$ is $\Theta(\log n)$
\end{itemize}
\underline{Properties}:
\begin{itemize}
    \item Given $f, g, h: \mathbb{N} \mapsto \mathbb{R}^+$, if $f(n)$ is $O(g(n))$ and $g(n)$ is $O(h(n))$, then $f(n)$ is $O(h(n))$
    \item Given $f_1, f_2, g_1, g_2: \mathbb{N} \mapsto \mathbb{R}^+$, if $f_1(n)$ is $O(g_1(n))$ and $f_2(n)$ is $O(g_2(n))$ then:
    \begin{itemize}
         \item $f_1(n) + f_2(n)$ is $O(g_1(n) + g_2(n))$ and $O(\max\{g_1(n), g_2(n)\})$
         \item $f_1(n)f_2(n)$ is $O(g_1(n)g_2(n))$ \\[0.75ex]
         Proof: There exist $n_1, C_1$ such that $\forall n \geq n_1 (f_1(n) \leq C_1 g_1(n))$ and there exist $n_2, C_2$ such that $\forall n \geq n_2 (f_2(n) \leq C_2 g_2(n))$. Let $n' = \max \{n_1, n_2\}$ and $C' = C_1C_2$, So for all $n \geq n', f_1(n)f_2(n) \leq C' g_1(n)g_2(n)$
    \end{itemize}
\underline{Comparing logarithms, polynomials, and exponential functions}: 
\begin{enumerate}
    \item $\log n^{C'}$ is $O(n^C)$
    \item $n^{O(1)}$ means at most $n^C$ for some $C > 0$. This is polynomial time 
    \item $2^{O(n)}$ means at most $2^{Cn}$ for some $C > 0$. This is exponential time. 
\end{enumerate}
\end{itemize}
\textbf{Lecture 3: Greedy Algorithms} \\[1.0ex]
\underline{Interval Scheduling Problem 1} (Maximizing number of jobs): Given $n$ jobs with a set time interval $[a_i, b_i]$ and one processor, find a schedule $S$ which accepts the maximum number of jobs without having two jobs running at the same time. Formally, we say that a sequence $S = (i_1, \dots , i_m)$ is a valid schedule if for all $j \in [m - 1](b_{i_j} \leq a_{i_{j + 1}})$ \\[1.0ex]
\underline{Greedy Algorithm for Interval Scheduling 1}: When choosing the next job, always choose the job which can be finished first.
\begin{itemize}
    \item \textit{Stored Data}: $S_k = (i_1, \dots, i_k)$ of the jobs which have been accepted thus far, and the time $t_k = b_{i_k}$
    \item \textit{Initialization}: $S_0 = \emptyset$ and $t_0 = 0$
    \item \textit{Iterative Step}: Choose the next job $i_{k + 1}$ from the set $\{i \in [n]\ |\ a_i \geq t_k = b_{i_k}\}$ such that $b_{i_k}$ is minimized. Update $S_{k + 1}$ and $t_{k + 1} = b_{i_{k + 1}}$, if there are no available jobs left then the algorithim terminates and we take $S = S_k$
\end{itemize}
\underline{Properties}: 
\begin{itemize}
    \item Given a valid schedule $S = (i_1, \dots, i_m)$, for all $j \in [m]$, we denote the time at which the jth hob in $S$ finishes as $t_j(S) = b_{i_j}$. For all $j > m$, we say $t_j(S) = \infty$
    \item If the greedy algorithim gives a valid schedule $S = (i_1, \dots, i_m)$, then for any other valid schedule $S' = (i_1, \dots, i_m'),\\ m' \leq m$ \\[0.5ex]
    Proof (Using the lemma proved below): Consider $j = m + 1, t_{m + 1} (S) = \infty$ so we must have that $t_{m + 1}(S') = \infty$. Thus, $m' \leq m$  
    \item If $S = (i_1, \dots, i_m)$ is the schedule given by the greedy algorithm then for any other valid schedule $S' = (i_1, \dots, i_m'),$ for all $j \in \mathbb{N}$, either $t_j(S) \leq t_j(S')$ or $t_j(S') = \infty$ \\[0.5ex]
    Proof: By induction on $j$. For the inductive step, assume that $t_k(S) \leq t_k(S')$. Note that $a_{i'_{k + 1}} \geq t_k(S') \geq t_k(S)$ so job $i'_{k + 1}$ is available to $S$. Since $S$ always chooses the job with the earliest completion time out of the available jobs, $t_{k + 1}(S) = b_{i_{k + 1}} \leq b_{i'_{k + 1}} = t_{k + 1}(S')$. 
    \item This algorithm can be implemented in $O(n \log n)$ time by first sorting the jobs in increacing order of $b_i$
\end{itemize}
\underline{Interval Scheduling Problem 2} (Mimimizing maximum lateness): Given $n$ jobs, each of which has length $l_i > 0$ and a deadline $d_i$, complete the jobs one by one such that the maximum lateness of any single job is minimized. Formally, define $t_S(i_k) = \sum_{j = 1}^{k}l_{i_j}$ to be the time at which job $i_k$ is finished, and find a schedule $S = (i_1, \dots, i_n)$ such that $\underset{{j \in [n]}}{\max}\{t_S(i_j) - d_{i_j}\}$ is minimized.
\underline{Greedy Algorithm for Interval Scheduling 2} (Earliest Deadline First): Take the schedule $S_{greedy}$ such that $d_{i_1},\dots,d_{n_1}$ are in increacing order. \\[1.0ex]
\underline{Properties}:
\begin{itemize}
    \item The schedule $S$ such that $d_{i_1},\dots,d_{n_1}$ are in increacing order minimizes $\underset{{j \in [n]}}{\max}\{t_S(i_j) - d_{i_j}\}$ \\
    Proof: Given an optimal schedule $S$ that is not $S_{greedy}$, we can obtain $S_{greedy}$ by exchanging pairs of jobs without increacing maximum lateness. Therefore, $S_{greedy}$ is optimal.
    \item If $S$ is an optimal schedule, and $d_{i_j} > d_{i_{j + 1}}$ then the schedule $S'$ obtained by swapping $i_j$ and $i_{j + 1}$ is also optimal.\\
    Proof: 
    \begin{enumerate}
        \item Swapping $i_j$ and $i_{j + 1}$ does not affect the lateness of any other job as for all $j' \in [n] \setminus \{j, j + 1\}$, $t_{S'}(i_{j'}) = t_{S}(i_{j'})$
        \item $t_S(i_{j + 1}) = t_{S'}(i_{j}) = t_{S'}(i_{j + 1}) + l_{i_j}$ as schedule $S'$ puts job $i_{j + 1}$ before job $i_j$
    \end{enumerate}
    This implies that $t_S(i_{j + 1}) - d_{i_{j + 1}} > t_{S'}(i_{j + 1}) - d_{i_{j + 1}} $ and  $t_S(i_{j + 1}) - d_{i_{j + 1}} > t_{S'}(i_{j}) - d_{i_{j}} $ as $d_{i_j} > d_{i_{j + 1}}$. Thus, the lateness of job $i_{j + 1}$ for schedule $S$ is greater than the lateness of both job $i_j$ and $i_{j + 1}$ for schedule $S'$. So the maximum lateness for $S'$ is less than or equal to the maximum lateness of $S$.
\end{itemize}

\textbf{Lecture 4: Dijkstra's Algorithm} \\[1.0ex]
\underline{Shortest Path Problem}: Given a directed graph $G = (V, E)$ with edge lengths $\{\ell_{uv}: (u, v) \in E \}$, a starting vertex $s$, and a destination vertex $t$, find the shortest path from $s$, to $t$.

\underline{Properties}: 
\begin{itemize}
    \item If the edge lengths are non-negative, at each step, Dijkstra's algorithm correctly computes the distance from $s$ to each vertex $u \in S$ \\
    Proof: By induction on .. Inductive step: assume the distances $\{d_u : u \in S\}$ are correct. Let $(u, v)$ be the next edge that Dijkstra's algorithm considers where $v \notin S$. This gives a path $P$ of length $d_u + \ell_{uv}$. If $P'$ is another path from $s$ to $v$, let $(u', v')$ be the first edge on $P'$ leaving $s$. $d_{u'} + \ell_{u'v'} \geq d_u + \ell_{uv}$, so the length of $P'$ is greater than or equal to $P$.
    \item 
\end{itemize}


\begin{algorithm}
\caption{Dijkstra's Algorithm}

\end{algorithm}

% Let $S \subseteq V$ be the set of explored nodes, and $E_{unexplored}$ be the set of unexplored edges. 
% Initially, $S = \{s\}$, $E_{unexplored} = \{(s, v)\}$ and $d_s = 0$
% While $S \neq V$ 
%     Choose the edge $e = (u, v) \in E_{unexplored}$ which minimizes $d_u + \ell_{uv}$ 
    
%     If $v = t$, we've found the shortest path. 
%     If $v \notin S$, we add $v$ to $S$ and set $d_v = d_u + \ell_{uv}$, and add $v's$ edges to $E_{unexplored}$


\textbf{Lecture 6: Minimum Spanning Trees} \\[1.0ex]
\underline{Minimum Spanning Tree}: Given a graph $G = (V, E)$, the mimimum spanning tree is a set of edges $T \subseteq E$ such that $(V, T)$ is a tree and $\sum_{e \in T} \ell_e$ is minimized.\\[1.0ex]
\underline{Kruskal's Algorithm}: Sort the edges $E$ in order of their length. We then go through the edges one by one and take each edge which does not form a cycle.
\begin{itemize}
    \item Stored Data: A set $T$ of the edges taken thus far 
    \item Iterative Step: We take the next edge $e \in E$. If $T \cup \{e\}$ contains a cycle, discard $e$, otherwise add $e$ to $T$. The algorithm terminates when $|T| = n - 1$ or there are no edges left to consider
\end{itemize}

Determining if $T \cup \{e\}$ creates a cycle can be done in $O(\alpha(n))$ time via union-find where $\alpha(n)$ is the inverse Ackermann function. \\[1.0ex]
\underline{Prim's Algorithm}: Iterativley accepts the shortest edge which leads to a new vertex.
\begin{itemize}
    \item Stored Data: A set $T$ of edges and a set $S$ of verticies taken thus far, and a set of edges $E_{candidate}$ with their lengths $\ell_e$
    \item Iterative Step: Remove the shortest edge $e = \{v, w\} \in E_{candidate}$. If $w$ is not in $S$, then we add $w$ to $S$ and $e$ to $T$, and update $E_{candidate}$ with all of $w$'s edges. The algorithm terminates when $S = V$ or $E_{candidate}$ is empty (in which case $G$ is not connected).
\end{itemize}
A cut is a partition $(L, R)$ of the verticies of $G$ where $L \neq \emptyset$ and $R \neq \emptyset$. We sat ab edge $e = \{u, v\}$ crosses the cut $(L, R)$ if $u$ is in $L$ and $v$ is in $R$ or vice versa. \\[0.5ex]
\underline{Theorem}: For each edge $e \in E$, $e$ is in the MST if and only if there exist a cut $(L, R)$ such that $e$ is the shortest edge crossing $(L, R)$\\[0.5ex]
Proof: If $e$ is the shortest edge crossing $(L, R)$, assume $e \notin T$. If so, adding $e$ creates a cycle, which contains some other edge $e'$ crossing $(L, R)$. So $T' = (T \cup \{e\}) \setminus \{e'\}$ has shorter total length than $T$, a contradiction. \\
If $e = \{u, v\} \in T$, take $L$ to be the set of verticies reachable from $u$ using the edges $T \setminus \{e\}$. If there were a shorter edge $e'$ crossing $(L, R)$, $T' = (T \cup \{e'\}) \setminus \{e\}$ would have shorter total length. \\[1.0ex]
\underline{Theorem}: For all edges $e \in E$, $e \notin T$ if and only if there exists a cycle $C$ such that $e$ is the longest edge of $C$ \\[1.0ex]
\underline{Correctness of Kruskal's Algorithm}: If $e$ is the shortest edge crossing a $(L, R)$, adding $e$ cannot create a cycle so $e$ will be taken. If Kruskal's algorithm takes $e = \{u, v\}$, let $T$ be the set of edges which were taken so far. Take $L$ to be the set of verticies reachable from $u$ with the edges in $T$. There cannot be a shorter edge $e'$ crossing $(L, R)$, as otherwise $e'$ would've been considered before $e$. \\[1.0ex]
\underline{Correctness of Prim's Algorithm}: Observe that at each step, Prim's algorithim takes the shortest edge crossing the cut between $S$ and the remaining verticies. So Prim's algorithim only takes edges in $T$. If $G$ is connected, the output of Prim's algorithim bust be connected. So all edges of $T$ must be taken. 

\end{document}